{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SN netuid - sn_name demo\n",
    "\n",
    "This demo notebook should fufill the following tasks.\n",
    " - (A) Demonstrate the quality of the communication between miners and validators coming from the top miner.\n",
    " - (B) Justify the difference in incentive for miners in different tiers (eg.quantile 1 VS quantile 3).\n",
    " - (C) (If applicable) Show the landscape and variety of miners. \n",
    " - (D) (If applicable) Demonstrate the effectiveness of the scoring mechanism.\n",
    " - (E) (If applicable) Show the dataset that was used by the validator.\n",
    " - (F) (If applicable) Show the use of any API and/or links to a frontend.\n",
    " \n",
    "- ** If you have difficulty in completing any of the task and need an alternative, please feel free to contact Isabella(isabella618033)/ Eugene(eugene3684) from the Opentensor Foundation on Discord.\n",
    "- ** We understand that SNs runs very differently, so please feel free to make any modification to the notebook that best suit your SN as long as it can demonstrate\n",
    "- ** For any wallet/ dendrite calls needed, we will be using the foundation hotkey \n",
    "\n",
    "## Objective\n",
    ">  Please write the objective of the SN with precision. \n",
    "> - The task of the miner (what task does the validator send to miner, and what are miner supposed to response with.)\n",
    "\n",
    "The miner has one main task that is to host and make data avialable, broken into three sub-tasks: to (1) `store`, data, (2) to pass random `challenges` to prove those data still exist, and (3) `retrieve` requested data. Thus, each miner has three axon endpoints, one for each of these tasks.\n",
    "\n",
    "FileTAO's major objective is to be a generic, agnostic data storage platform for Bittensor, the surrounding ecosystem, and non-crypto participants as well.  Storing any data encrypted end-to-end in a decentralized, robust protocol so that users can reduce dependencies on third-party centralized actors is paramount. For example, one major goal of FileTAO is to provide storage for backing up chain data, machine learning model weights, structured and unstructured data, machine learning training data, end-user files, and more while providing levels of redundancy and data saliency.\n",
    "\n",
    "There are several components to SN21's mechanism:\n",
    "(1) Cryptographic Proof System: how do we verify the data still exists and miners have data integrity.\n",
    "(2) Reward System: How to we distribute rewards fairly and meritocratically while minimizing unnecessary churn.\n",
    "(3) Data Preservation & Recovery: How do we rebalance the data on the network so that no data loss is observed, even if/when some nodes fail.\n",
    "\n",
    "\n",
    "### Store\n",
    "In the Store phase, the goal is to securely store data and create a commitment to prove its storage without revealing the data itself. The mathematical steps are:\n",
    "\n",
    "Validators query miners to store user data that is encrypted by the end-user coldkey pubkey. The encrypted data is sent to miners along with a random seed, which the miners use to initiate a sequential chain of seed + hash verification proofs. The previous seed value and the data is required for each subsequent commitment proof. The miner then applies a Pedersen committment to the entire data using the random seed, and forwards the proofs to the validator.\n",
    "\n",
    "Upon receipt, validators verify the commitment and the initial hash chain, storing the associated metadata.\n",
    "\n",
    "1. **Data Receiving**\n",
    "   - Base64 decode incoming bytes data stream (saves bandwidth in transit)\n",
    "   - Hash the data with `sha256` for creating a file ID to store in the index that matches the validator file ID\n",
    "   - Save the data to the filesystem and save associated metadata, including data `size` (bytes), validator random `seed`, and elliptic curve initialization parameters `(g,h)`.\n",
    "\n",
    "2. **Data Encryption**:\n",
    "   - Data `D` is encrypted using a symmetric encryption scheme whos keys are private to the client. The miner cannot decrypt the data or know what it is receiving.\n",
    "   - Encrypted Data: `encrypted_D = encrypt(D, key)`.\n",
    "\n",
    "3. **Hashing and Commitment**:\n",
    "   - Hash the encoded data with a unique random seed to create a unique identifier for the data.\n",
    "   - Data Hash: `data_hash = hash(encrypted_D + r_seed)`.\n",
    "   - Create a cryptographic commitment using an Elliptic Curve Commitment Scheme (ECC), which involves a commitment function `commit` with curve points `g` and `h`.\n",
    "   - Pedersen Commitment: `(c, m, r) = commit(encrypted_D + seed)`, where `c` is the commitment, `m` is the message (or commitment hash), and `r` is the randomness used in the commitment.\n",
    "   - Chained Hash Proof: `m` is used as the initial `C_0`, which contains the initial random seed and the data itself. The random seed is stored for the next challenge in the chain.\n",
    "\n",
    "4. **Storage**:\n",
    "   - Store the data (`E`) and the random seed (`r_seed`) in local storage.\n",
    "\n",
    "5. **Response** \n",
    "   - Return randomness value `r`, commitment point `c`, produced by the Pedersen commitment, along with commitment hash `m` (a.k.a `C_0`) to the calling validator.\n",
    "\n",
    "\n",
    "### Challenge\n",
    "\n",
    "In the Challenge phase, the system verifies the possession of the data without actually retrieving the data itself.\n",
    "\n",
    "Validators request the miner prove that it currently stores the data claimed by issuing an index-based challenge, where the miner must apply Pedersen commitments to the entire data table given a random seed and a chunk size.\n",
    "\n",
    "Data is chunked according to the chunk size, and each slice is committed to using a Pederson commitment with the current random seed. Each commitment is appended to a merkle tree, and a subsequent proof is generated to obtain the path along the merkle tree such that a validator can verify the random seed was indeed used to commit to each data chunk at challenge time, thus proving the miner has the data at time of the challenge. \n",
    "\n",
    "The mathematical operations involved in the `Challenge` phase of the data storage and verification protocol can be broken down into a few key steps. Here's a simplified explanation:\n",
    "\n",
    "1. **Chunking Data**:\n",
    "   - The encrypted data is split into chunks: `chunks = chunk(encrypted_D, chunk_size)`.\n",
    "\n",
    "2. **Selecting a Chunk for Challenge**:\n",
    "   - A random chunk is selected for the challenge.\n",
    "   - Selected Chunk: `chunk_j = chunks[j]`.\n",
    "\n",
    "3. **Computing Commitment for the Chunk**:\n",
    "   - A commitment is computed for the selected chunk.\n",
    "   - Commitment for Chunk: `(c_j, m_j, r_j) = commit(chunk_j + seed)`.\n",
    "\n",
    "4. **Creating a Merkle Tree**:\n",
    "   - A Merkle tree is constructed using all chunk commitments.\n",
    "   - Merkle Tree: `merkle_tree = MerkleTree([c_1, c_2, ..., c_n])`.\n",
    "\n",
    "5. **Generating Merkle Proof**:\n",
    "   - A Merkle proof is generated for the selected chunk to recreate the path along the merkle tree to the leaf that represents `chunk_j`.\n",
    "   - Merkle Proof: `proof_j = merkle_tree.get_proof(j)`.\n",
    "\n",
    "6. **Generating chained commitment**:\n",
    "   - Compute commitment hash `Cn = hash( hash( encrypted_D + prev_seed ) + synapse.seed )`\n",
    "   - Update previous seed `prev_seed = synapse.seed`\n",
    "\n",
    "7. **Response**:\n",
    "   - The challenge response includes the Pedersen elliptic curve commitment, the chained commitment hash, the Merkle proof, and the Merkle tree root.\n",
    "   - The validator verifies the triple of proofs: chained commitment, elliptic-curve commitment, and the merkle proof.\n",
    "\n",
    "\n",
    "### Retrieve\n",
    "In this phase, the data is retrieved, decrypted, and its integrity is verified. This is achieved by the validator supplying a random hash to check that is owned by the miner, along with a random seed to incorporate.\n",
    "\n",
    "1. **Fetching Encrypted Data**:\n",
    "   - The encrypted data is fetched from the database based on its hash.\n",
    "   - `encrypted_D = fetch(data_hash)`.\n",
    "\n",
    "2. **Chained Verification Challenge**:\n",
    "   - A new commitment is computed on the encrypted data with a new seed and the previous seed.\n",
    "       - `Ch = hash( hash( encrypted_D + prev_seed ) + synapse.seed )`.\n",
    "\n",
    "3. **Data Integrity Check**:\n",
    "   - The retrieved data's integrity is verified by checking if the newly computed commitment matches the expected value.\n",
    "   - `verify_chained(commitment, expected_commitment) == True`.\n",
    "\n",
    "4. **Final Data Commitment Proof**\n",
    "   - Pedersen Commitment: `(c, m, r) = commit(encrypted_D + seed)`, where `c` is the commitment, `m` is the message (or commitment hash), and `r` is the randomness used in the commitment.\n",
    "\n",
    "5. **Submission**:\n",
    "   - Return data `D`, `Ch` chained hash, `r` randomness value, and `c` commimtment point.\n",
    "   - Data is validated by proofs and decrypted on validator side to be sent back to the end-user: `D = decrypt(encrypted_D, key)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> - The scoring mechanism (please specify is there is any model involved)\n",
    "\n",
    "There are several components to the reward mechanism and is multi-layered. In a nutshell the goals are as follows:\n",
    "\n",
    "- Incentivize data durability and salience via cryptoraphic proof system. (E.g. don't lose shit.)\n",
    "- Incentivize good performance over a long period of time. (Tier system to reward based on reputation.)\n",
    "- Incentivize fast response times. We want to retrieve user data quickly and ensure that it is over high network bandwidth.\n",
    "\n",
    "### Cryptographic Proof System:\n",
    "See above description for details on the algorithm. It is rewarded as follows:\n",
    "```\n",
    "\n",
    "SUPER_SAIYAN_TIER_REWARD_FACTOR = 1.0\n",
    "DIAMOND_TIER_REWARD_FACTOR = 0.9\n",
    "GOLD_TIER_REWARD_FACTOR = 0.8\n",
    "SILVER_TIER_REWARD_FACTOR = 0.7\n",
    "BRONZE_TIER_REWARD_FACTOR = 0.6\n",
    "```\n",
    "\n",
    "### Tier System:\n",
    "The tier system classifies miners into five distinct categories, each with specific requirements and storage limits. These tiers are designed to reward miners based on their performance, reliability, and the total volume of data they can store.\n",
    "\n",
    "Importance of Tier System:\n",
    "- **Encourages High Performance:** Higher tiers reward miners with greater benefits, motivating them to maintain high Wilson Scores.\n",
    "- **Enhances Network Reliability:** The tier system ensures that only the most reliable and efficient miners handle significant volumes of data, enhancing the overall reliability of the network.\n",
    "- **Fair Reward Distribution:** The reward factors are proportional to the miners' tier, ensuring a fair distribution of rewards based on performance.\n",
    "\n",
    "1. 🎇 **Super Saiyan Tier:** \n",
    "   - **Storage Limit:** 1 Exabyte (EB)\n",
    "   - **Store Wilson Score:** 0.88\n",
    "   - **Minimum Successes Required:** 10,000\n",
    "   - **Reward Factor:** 1.0 (100% rewards)\n",
    "\n",
    "2. 💎 **Diamond Tier:**\n",
    "   - **Storage Limit:** 1 Petabyte (PB)\n",
    "   - **Store Wilson Score:**  0.77\n",
    "   - **Minimum Successes Required:** 5,000\n",
    "   - **Reward Factor:** 0.9 (90% rewards)\n",
    "\n",
    "3. 🥇 **Gold Tier:**\n",
    "   - **Storage Limit:** 100 Terabytes (TB)\n",
    "   - **Store Wilson Score:** 0.66\n",
    "   - **Minimum Successes Required:** 2,000\n",
    "   - **Reward Factor:** 0.8 (80% rewards)\n",
    "\n",
    "4. 🥈 **Silver Tier:**\n",
    "   - **Storage Limit:** 10 Terabytes (TB)\n",
    "   - **Store Wilson Score:** 0.55\n",
    "   - **Minimum Successes Required:** 1,000\n",
    "   - **Reward Factor:** 0.7 (70% rewards)\n",
    "\n",
    "5. 🥉 **Bronze Tier:**\n",
    "   - **Storage Limit:** 1 Terabyte (TB)\n",
    "   - **Store Wilson Score:** Not specifically defined for this tier\n",
    "   - **Minimum Successes Required:** Not specifically defined for this tier\n",
    "   - **Reward Factor:** 0.6 (60% rewards)\n",
    "\n",
    "#### Maintaining and Advancing Tiers:\n",
    "- To advance to a higher tier, miners must consistently achieve the required minimum Wilson Scores in their operations.\n",
    "- Periodic evaluations are conducted to ensure miners maintain the necessary performance standards to stay in their respective tiers, or move up or down tiers.\n",
    "- Advancing to a higher tier takes time. In order to ascend to the first higher tier (Silver), it takes at least 1000 successful requests, whether they are challenge requests, store requests, or retry requests and must maintain the minimum Wilson Score for successes / attempts. \n",
    "- Depending on how often a miner is queried, how many validators are operating at one given time, and primarily the performance of the miner, this can take several hours to several days. Assuming full 64 validator slots are occupied, this should take ~100 hours.\n",
    "\n",
    "As miners move up tiers, their responsibility increases proportionally. Thus, miners who move from Silver -> Gold are expected to be able to store up to 100 Terabytes (TB) of data, a 10x the storage cap for Silver. Similarly, it must maintain a minimum wilson score of 0.66 over it's lifetime, increasing the lower bound for expected performance and reliability. It also means that this miner will now recieve a higher percentage of rewards for each successful query, going from 70% -> 80%. This does not mean that the miner may rest on its laurels, and may move right back down a tier (or more) if it does not meed the minimum requirements for Gold.\n",
    "\n",
    "### Wilson score\n",
    "Wilson score is an asymmetric approximation of the confidence interval suited for low sample sizes, given a particular z-score target. It doesn't suffer from problems of overshoot and zero-width intervals that afflict the normal interval approximation. \n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval for more information.\n",
    "\n",
    "### Tier Ascension Time\n",
    "Assuming perfect performance, that out of ~200 miner UIDs, each of which is queried roughly 34 times every 1000 rounds, namely a 3.4% chance every query round, one can expect to reach the next tier within \n",
    "```bash\n",
    "hours = total_successes / prob_of_query_per_round * time_per_round / 3600\n",
    "hours = 98 # roughly 4 days at perfect performance to next (Silver) tier (assuming no challenge failures)\n",
    "```\n",
    "\n",
    "#### Miner Advancement Program\n",
    "As a strategy for rewarding mineres who perform well but are lower tiers to move more quickly up through the ranks, we apply an inverse boosting strategy for the top miners per query batch. The top two (2) performers per batch of `store`, `challenge` or `retrieve` requests recieve a one-time tier appropriate boost. These tier-specific boosts decrease as you ascend the tier structure, and are defined in `constants.py`. For example:\n",
    "\n",
    "```bash\n",
    "TIER_BOOSTS = {\n",
    "    b\"Super Saiyan\": 1.02, # 2%  -> 1.02\n",
    "    b\"Diamond\": 1.05,      # 5%  -> 0.945\n",
    "    b\"Gold\": 1.1,          # 10% -> 0.88\n",
    "    b\"Silver\": 1.15,       # 15% -> 0.805\n",
    "    b\"Bronze\": 1.2,        # 20% -> 0.72\n",
    "}\n",
    "```\n",
    "\n",
    "Concretely, a `Bronze` miner who is one of the top 2 within a batch of requests will receive a proportionally larger boost than a `Diamond` miner who is also in the top 2.\n",
    "\n",
    "```\n",
    "REWARD = TIER * REWARD * BOOST\n",
    "Bronze  -> 0.72 = 0.6 * 1.0 * 1.2\n",
    "Diamond -> 0.84 = 0.8 * 1.0 * 1.05\n",
    "```\n",
    "\n",
    "This mechansim *significantly* closes the gap for newer miners who perform well and should be able to ascned the tier structure honestly and faithfully. Essentially is so that miners who consistently perform well but are lower tiers can more readily survive immunity period to make it to successively higher tiers and not \"gate\" access to the older mineres. This directly negates the \"grandfathering\" effect. Higher tier miners that are in the top 2 are boosted significantly less than those Bronze or lower tier miners who make the top 2.\n",
    "\n",
    "#### Periodic Statistics Rollover\n",
    "\n",
    "Statistics for `store_successes/attempts`, `challenge_attempts/successes`, and `retrieve_attempts/successes` are reset every 2 epochs (720 blocks), while the `total_successes` are carried over for accurate tier computation. This \"sliding window\" of the previous 720 blocks of `N` successes vs `M` attempts effectively resets the `N / M` ratio and applies Wilson Scoring. This facilitates a less punishing tier calculation for early failures that would otherwise have to be \"outpaced\", while simultaneously discourages grandfathering older miners who were able to succeed early and cemented their status in a higher tier. The net effect is greater mobility across the tiers, keeping the network competitive while incentivizing reliability and consistency.\n",
    "\n",
    "For example:\n",
    "```bash\n",
    "store_successes = 2\n",
    "store_attempts = 2\n",
    "challenge_successes = 3\n",
    "challenge_attempts = 5\n",
    "retrieve_successes = 1\n",
    "retireve_attempts = 1\n",
    "total_successes_epoch = 7\n",
    "total_attempts_epoch = 9\n",
    "total_successes = 6842\n",
    "wilson_score = 0.79\n",
    "```\n",
    "This miner would qualify for Diamond tier this round, as it has passed the minimum threshold for total successes (5000) and wilson score (0.77).\n",
    "\n",
    "\n",
    "> - The dataset used\n",
    "None. Client and challenge data are intermingled. Thus, miners cannot determine if data is from a validator or from a client, forcing miners to respond to any and *all* requests by validators, as it directly impacts their incentive, regardless of random challenge or client API data.\n",
    "\n",
    "## Setup\n",
    "> Please give instruction for how to run the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Top miner responses\n",
    "- This section should demonstrate the quality of the communication between miners and validators coming from the top miner.\n",
    "\n",
    "> - (1) Define the group of top miners.\n",
    "The Top miner group is typically the highest tier, (but not exclusively), also have the highest incentive (roughly top 15% of miners), and provide the best service for the end goal of data storage and retrieval. The top miner group nearly always succeeds in challenges, and clients/validators are able to retrieve data consistently, with low latency.\n",
    "\n",
    "> - (2) Define the forward function. \n",
    "There are three (3) distinct forward functions, as described in detail above: `store`, `challenge` and `retrieve`, all of which are rewarded or punished based on reponses from miners. The fundamental reward is binary based on answering this simple question: \"Did you, or did you not store (or still posess) this particular piece of data?\" A 0 or 1 answer, but is then modified by several factors, such as tier, latency, and relative position in the query group.\n",
    "\n",
    "Here is an example (simplified) description and pseudocode for the `challenge` forward:\n",
    "\n",
    "Miner handles a data challenge by providing a series of cryptographic proofs of data possession. This method retrieves\n",
    "the specified data from storage, calculates its commitment using elliptic curve cryptography, and\n",
    "constructs a Merkle proof. The response includes the requested data chunk, Merkle proof, root, and\n",
    "the commitment, which collectively serve as verifiable evidence of data possession.\n",
    "\n",
    "The method performs the following steps:\n",
    "1. Fetches the encrypted data from storage using the hash provided in the challenge.\n",
    "2. Splits the data into chunks based on the specified chunk size.\n",
    "3. Computes a new commitment hash to provide a time-bound proof of possession.\n",
    "4. Generates a Merkle tree from the committed data chunks and extracts a proof for the requested chunk.\n",
    "5. Encodes the requested chunk and Merkle proof in base64 for transmission.\n",
    "6. Updates the challenge synapse with the commitment, data chunk, randomness, and Merkle proof.\n",
    "7. Records the updated commitment hash in storage for future challenges.\n",
    "\n",
    "This method ensures data integrity and allows the verification of data possession without disclosing the\n",
    "entire data. It is designed to fulfill data verification requests in a secure manner with minimal overhead.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "async def challenge(synapse: Challenge) -> Challenge:\n",
    "    # Retrieve the data itself from miner storage\n",
    "    data = await get_chunk_metadata(database)\n",
    "\n",
    "    # Chunk the data according to the specified (random) chunk size \n",
    "    encrypted_data_bytes = load_from_filesystem(data[\"filepath\"])\n",
    "\n",
    "    # Construct the next commitment hash using previous commitment and hash\n",
    "    # of the data to prove storage over time\n",
    "    prev_seed = data[\"seed\"]\n",
    "\n",
    "    # Compute \"chained hash commitmenet\" given prev_seed, data and new_seed\n",
    "    next_commitment, proof = compute_subsequent_commitment(encrypted_data_bytes, prev_seed, synapse.seed)\n",
    "\n",
    "    # Store the values back in the system to return to the validator for verificatin.\n",
    "    synapse.commitment_hash = next_commitment\n",
    "    synapse.commitment_proof = proof\n",
    "\n",
    "    # update the commitment seed challenge hash in metadata storage\n",
    "    await update_seed_info(\n",
    "        database,\n",
    "        chunk_hash=synapse.challenge_hash,\n",
    "        hotkey=synapse.dendrite.hotkey,\n",
    "        seed=synapse.seed,\n",
    "    )\n",
    "\n",
    "    # Chunk the data according to the provided chunk_size\n",
    "    data_chunks = chunk_data(encrypted_data_bytes, synapse.chunk_size)\n",
    "\n",
    "    # Extract setup params (initial points along the curve)\n",
    "    g = hex_to_ecc_point(synapse.g, synapse.curve)\n",
    "    h = hex_to_ecc_point(synapse.h, synapse.curve)\n",
    "\n",
    "    # Commit the data chunks based on the provided curve points\n",
    "    randomness, chunks, commitments, merkle_tree = commit_data_with_seed(\n",
    "        ECCommitment(g, h),\n",
    "        data_chunks=data_chunks,\n",
    "        n_chunks=size(encrypted_data_bytes) // synapse.chunk_size + 1,\n",
    "        seed=synapse.seed,\n",
    "    )\n",
    "\n",
    "    # Prepare return values to validator for proper proof verification\n",
    "    # Includes: commitment point, randomness value, and merkle proof up to leaf node of index `j`\n",
    "    # Base64 encode the data chink and the merkle proof for compression during transit.\n",
    "    synapse.commitment = commitments[synapse.challenge_index]\n",
    "    synapse.data_chunk = base64.b64encode(chunks[synapse.challenge_index])\n",
    "    synapse.randomness = randomness[synapse.challenge_index]\n",
    "    synapse.merkle_proof = b64_encode(\n",
    "        merkle_tree.get_proof(synapse.challenge_index)\n",
    "    )\n",
    "\n",
    "    synapse.merkle_root = merkle_tree.get_merkle_root()\n",
    "    return synapse\n",
    "\n",
    "```\n",
    "\n",
    "> - (3) Call the forward function for the top miners. \n",
    "The forward function is not designed to be called on the \"top\" miners specifically, but rather on only the miners who have stored the piece (or pieces) of data desired for both `challenge`s and `retrieve`s.\n",
    "\n",
    "In FileTao it is not useful to query the \"top\" miner set, but rather use the validator as a proxy to store and retrieve data on the miner set. E.g. you cannot store data directly on miners without a registered validator hotkey, and if you were to attempt to retrieve data you would need to know specific CIDs and miner hotkeys where those data are stored. Instead we should query the validator using `StoreUser` and `RetrieveUser` `synapse`s. See the API example below in section  for more details.\n",
    "\n",
    "However, if desiring to only query the top miners for the purposes of this evaluation, one can easily call `Store` on the highest tier miners at the current time given a registered validator hotkey. For example, attempting to store and then retrieve a specific piece of data from the network given it's content identifier. This will require some low-level functions to import and use.\n",
    "\n",
    "\n",
    "```python\n",
    "import base64\n",
    "import numpy as np\n",
    "import bittensor as bt\n",
    "from redis import asyncio as aioredis\n",
    "from Crypto.Random import get_random_bytes\n",
    "from storage import protocol\n",
    "from storage.shared.utils import get_redis_password\n",
    "from storage.validator.database import *\n",
    "from storage.shared.ecc import (\n",
    "    hash_data,\n",
    "    setup_CRS,\n",
    "    ecc_point_to_hex,\n",
    ")\n",
    "\n",
    "\n",
    "data = b\"Some bytes data to store on the network!\"\n",
    "metagraph = bt.subtensor(\"test\").metagraph(netuid=22, lite=False)\n",
    "\n",
    "# Grab top 10% of miners by incentive\n",
    "def get_top_miner_uids(n=0.1):\n",
    "    top_i = np.quantile(metagraph.I, 1 - n)\n",
    "    uids = metagraph.uids[metagraph.I > top_i]\n",
    "    return [uid.item() for uid in uids]\n",
    "\n",
    "axons = [metagraph.axons[uid] for uid in get_top_miner_uids()]\n",
    "\n",
    "# Setup CRS (common reference string) for this round of validation\n",
    "curve = \"P-256\"\n",
    "g, h = setup_CRS(curve=curve)\n",
    "\n",
    "# Hash the data\n",
    "data_hash = hash_data(data)\n",
    "\n",
    "# Convert to base64 for compactness\n",
    "b64_encrypted_data = base64.b64encode(data).decode(\"utf-8\")\n",
    "\n",
    "# Create Store synapse\n",
    "synapse = protocol.Store(\n",
    "    encrypted_data=b64_encrypted_data,\n",
    "    curve=curve,\n",
    "    g=ecc_point_to_hex(g),\n",
    "    h=ecc_point_to_hex(h),\n",
    "    seed=get_random_bytes(32).hex(),  # 256-bit seed\n",
    "    ttl=12345, # how many seconds before miners can safely delete\n",
    ")\n",
    "\n",
    "# Must use a registered validator hotkey wallet\n",
    "dendrite = bt.dendrite(wallet=bt.wallet())\n",
    "\n",
    "# Store the data on the top miner set\n",
    "store_responses = await dendrite(\n",
    "    axons,\n",
    "    synapse,\n",
    "    deserialize=False,\n",
    "    timeout=20,\n",
    ")\n",
    "\n",
    "# Now retrieve the data using the same CID (data_hash in this case) and miner set that we stored on.\n",
    "synapse = protocol.Retrieve(\n",
    "    data_hash=data_hash,\n",
    "    seed=get_random_bytes(32).hex(), # New seed\n",
    ")\n",
    "\n",
    "retrieve_responses = await dendrite(\n",
    "    axons,\n",
    "    synapse,\n",
    "    deserialize=False,\n",
    "    timeout=20,\n",
    ")\n",
    "\n",
    "rdata = retrieve_responses[0].data\n",
    "rdata = base64.b64decode(rdata)\n",
    "assert rdata == data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - (4) Show the responses from the miners.\n",
    "\n",
    "```python\n",
    "\n",
    "print(\"Store responses:\", store_responses)\n",
    "print(\"Retrieve responses:\", retrieve_responses)\n",
    "\n",
    "# Pick 1\n",
    "print(\"Single store response:\" store_responses[0])\n",
    "print(\"Single retrieve response:\" retrieve_responses[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Justification for incentive distribution \n",
    "- Justify the difference in incentive for miners in different incentive tiers (eg. sample 5 miners from quantile 1 VS 5 miners from quantile 3) with code.\n",
    "\n",
    "Miner incentive is largeley dependent on three factors: (1) tier, (2) latency, (3) success rate. Tier is an aggregation of the 2nd and 3rd qualities. The more often you consistently pass challenges as a miner and the lower your latency, the higher your tier will be over time.\n",
    "\n",
    "For example, let's imagine a set of miners was queried to store a given piece of data. The validator would create the `Store` synapse and send the query out, and use the responses to fill the rewards. Let's walk through this logic together:\n",
    "\n",
    "(1) `create_rewards_vector()` is called, verfying and applying the initial tier based rewards.\n",
    "Here is a simplified pseudocode implementation for explanation:\n",
    "\n",
    "```python\n",
    "# Define boosts for top 2 miners per query batch. Lower tier miners who perform well get boosted more.\n",
    "TIER_BOOSTS = {\n",
    "    b\"Super Saiyan\": 1.02, # 2%  -> 1.02\n",
    "    b\"Diamond\": 1.05,      # 5%  -> 0.945\n",
    "    b\"Gold\": 1.1,          # 10% -> 0.88\n",
    "    b\"Silver\": 1.15,       # 15% -> 0.805\n",
    "    b\"Bronze\": 1.2,        # 20% -> 0.72\n",
    "}\n",
    "\n",
    "async def create_reward_vector(\n",
    "    synapse: Union[Store, Retrieve, Challenge],\n",
    "    rewards: torch.FloatTensor,\n",
    "    uids: List[int],\n",
    "    responses: List[Synapse],\n",
    "):\n",
    "\n",
    "    # Sort the miner response times (ascending)\n",
    "    sorted_times = ...\n",
    "\n",
    "    # Get the top 2 lowest latency miners in this query batch and mark them for extra proportional boost\n",
    "    in_top_2_dict = {\n",
    "        uid: True if time < synapse.timeout else False\n",
    "        for (uid, time) in sorted_times[:2]\n",
    "    }\n",
    "\n",
    "    for idx, (uid, response) in enumerate(zip(uids, responses)):\n",
    "        # Verify the commitment\n",
    "        success = verify_fn(synapse=response)\n",
    "\n",
    "        ...\n",
    "\n",
    "        # Apply reward for this task\n",
    "        tier_factor = await get_tier_factor(hotkey, self.database)\n",
    "\n",
    "        # Boost the factor for the top 2 miner tiers by x% given their current tier\n",
    "        if in_top_2_dict.get(uid, False):\n",
    "            tier_factor *= TIER_BOOSTS[tier]\n",
    "\n",
    "        # Apply the reward based on tier factor\n",
    "        rewards[idx] = 1.0 * tier_factor if success else failure_reward * tier_factor\n",
    "\n",
    "    # Scale rewards according to latency\n",
    "    scaled_rewards = scale_rewards(\n",
    "        uids,\n",
    "        responses,\n",
    "        rewards,\n",
    "    )\n",
    "\n",
    "    # Scatter the rewards into the full 256 UID vector.\n",
    "    scattered_rewards: torch.FloatTensor = (\n",
    "        self.moving_averaged_scores.to(self.device)\n",
    "        .scatter(\n",
    "            0,\n",
    "            uids,\n",
    "            scaled_rewards,\n",
    "        )\n",
    "        .to(self.device)\n",
    "    )\n",
    "\n",
    "    # Update moving_averaged_scores with rewards produced by this step.\n",
    "    # shape: [ metagraph.n ]\n",
    "    alpha: float = 0.05\n",
    "    self.moving_averaged_scores: torch.FloatTensor = alpha * scattered_rewards + (\n",
    "        1 - alpha\n",
    "    ) * self.moving_averaged_scores\n",
    "\n",
    "```\n",
    "\n",
    "- If there is no significant difference in the incentive distribution, you can also show that miners in the SN have about the same performance in multiple ways.\n",
    "\n",
    "- There could be many reasons for the difference in incentive for miners. \n",
    "    - Case 1: Difference in the quality of response\n",
    "        - Show that miners with higher incentive generally give better answer then those with lower incentive through the following ways\n",
    "            - lower loss; higher accuracy\n",
    "            - human eval for text/ image/ audio quality \n",
    "\n",
    "The quality of response is not relevant here, simply because it is a binary outcome. If successful, the cryptographic proofs will be verifiable and return `True`. If the cryptographic verification fails for any reason, we either give `0` or `negative` rewards based on the request type and the miner tier.\n",
    "\n",
    "For completeness, here are verification algoritms for `challenge` proofs:\n",
    "```python\n",
    "\n",
    "def verify_chained_commitment(proof, seed, commitment, verbose=True):\n",
    "    \"\"\"\n",
    "    Verifies the accuracy of a chained commitment using the provided proof, seed, and commitment.\n",
    "    The function hashes the concatenation of the proof and seed and compares this result with the provided commitment\n",
    "    to determine if the commitment is valid.\n",
    "    Args:\n",
    "        proof (str): The proof string involved in the commitment.\n",
    "        seed (str): The seed string used in generating the commitment.\n",
    "        commitment (str): The expected commitment hash to validate against.\n",
    "        verbose (bool, optional): Enables verbose logging for debugging. Defaults to True.\n",
    "    Returns:\n",
    "        bool: True if the commitment is verified successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    if proof is None or seed is None or commitment is None:\n",
    "        return False\n",
    "\n",
    "    expected_commitment = str(hash_data(proof.encode() + seed.encode()))\n",
    "\n",
    "    return expected_commitment == commitment\n",
    "\n",
    "def validate_merkle_proof(proof, target_hash, merkle_root, hash_type=\"sha3_256\"):\n",
    "    \"\"\"\n",
    "    Validates a Merkle proof, verifying that a target element is part of a Merkle tree with a given root.\n",
    "\n",
    "    A Merkle proof is a sequence of hashes that, when combined with the target hash through the hash function\n",
    "    specified by `hash_type`, should result in the Merkle root if the target hash is indeed part of the tree.\n",
    "\n",
    "    Parameters:\n",
    "        proof (list of dicts): A list of dictionaries where each dictionary has one key, either 'left' or 'right',\n",
    "            corresponding to whether the sibling hash at that level in the tree is to the left or right of the path\n",
    "            leading to the target hash.\n",
    "        target_hash (str): The hexadecimal string representation of the target hash being proven as part of the tree.\n",
    "        merkle_root (str): The hexadecimal string representation of the Merkle root of the tree to which the target\n",
    "            hash is being validated against.\n",
    "        hash_type (str, optional): The type of hash function used to construct the Merkle tree. This must match the\n",
    "            hash function used in constructing the original Merkle tree. Defaults to \"sha3_256\", and it must be an\n",
    "            attribute of the `hashlib` module that takes a bytes object and returns a hash object that has a `digest`\n",
    "            method.\n",
    "\n",
    "    Example:\n",
    "        # Example of validating a Merkle proof\n",
    "        valid_proof = [{'left': 'abc...'}, {'right': 'def...'}]\n",
    "        target = 'a1b2c3...'\n",
    "        root = '123abc...'\n",
    "        is_valid = validate_merkle_proof(valid_proof, target, root)\n",
    "        print(is_valid)  # Outputs True if the proof is valid, False otherwise\n",
    "    \"\"\"\n",
    "    hash_func = getattr(hashlib, hash_type)\n",
    "    merkle_root = bytearray.fromhex(merkle_root)\n",
    "    target_hash = bytearray.fromhex(target_hash)\n",
    "    if len(proof) == 0:\n",
    "        return target_hash == merkle_root\n",
    "    else:\n",
    "        proof_hash = target_hash\n",
    "        for p in proof:\n",
    "            try:\n",
    "                # the sibling is a left node\n",
    "                sibling = bytearray.fromhex(p[\"left\"])\n",
    "                proof_hash = hash_func(sibling + proof_hash).digest()\n",
    "            except:\n",
    "                # the sibling is a right node\n",
    "                sibling = bytearray.fromhex(p[\"right\"])\n",
    "                proof_hash = hash_func(proof_hash + sibling).digest()\n",
    "        return proof_hash == merkle_root\n",
    "\n",
    "def verify_challenge_with_seed(synapse, seed, verbose=False):\n",
    "    \"\"\"\n",
    "    Verifies a challenge in a decentralized network using a seed and the details contained in a synapse.\n",
    "    The function validates the initial commitment hash against the expected result, checks the integrity of the commitment,\n",
    "    and verifies the merkle proof.\n",
    "    Args:\n",
    "        synapse (Synapse): The synapse object containing challenge details.\n",
    "        verbose (bool, optional): Enables verbose logging for debugging. Defaults to False.\n",
    "    Returns:\n",
    "        bool: True if the challenge is verified successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    if synapse.commitment_hash is None or synapse.commitment_proof is None:\n",
    "        return False\n",
    "\n",
    "    if not verify_chained_commitment(\n",
    "        synapse.commitment_proof, seed, synapse.commitment_hash, verbose=verbose\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    committer = ECCommitment(\n",
    "        hex_to_ecc_point(synapse.g, synapse.curve),\n",
    "        hex_to_ecc_point(synapse.h, synapse.curve),\n",
    "    )\n",
    "    commitment = hex_to_ecc_point(synapse.commitment, synapse.curve)\n",
    "\n",
    "    if not committer.open(\n",
    "        commitment,\n",
    "        hash_data(base64.b64decode(synapse.data_chunk) + str(seed).encode()),\n",
    "        synapse.randomness,\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    if not validate_merkle_proof(\n",
    "        b64_decode(synapse.merkle_proof),\n",
    "        ecc_point_to_hex(commitment),\n",
    "        synapse.merkle_root,\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "```\n",
    "\n",
    "Only if all three (3) proofs succeed, do we consider this response \"successful\". The challenged miner must provide:\n",
    "(1) A \"chained hash\" commitment proof. Contains the previous random seed chosen by the validator, the current seed chosed by the validator, and the original data. All three are required to pass this check.\n",
    "(2) A Pedersen commitment proof at data index `j` containing data chunk `D_j`. The miner commits to this specific data chunk (small, usually < 128 kb) and returns the commitment with the randomness value `r` to complete this proof.\n",
    "(3) A Merkle proof wherein the leaves of the merkle tree are the commitments (points) along the elliptic curve up to the index `j`, such that validators are able to validate the entire merkle tree up to index `j` without having to recieve any previous data or have special knowledge other than the merkle root, and the commitment for index `j`.\n",
    "\n",
    "If *any* of these fail, the entire request is considered a failure and gets zero or negative reward.\n",
    "\n",
    "- \n",
    "    - Case 2: Difference in miner avalibility \n",
    "        - Then you can show that given a certain number of trials(100), there are more successful calls to higher incentive miners.\n",
    "\n",
    "This information is encapsulated in two places: (1) the `Wilson score` and (2) the `monitor` protocol. \n",
    "\n",
    "Given `N` trials, the wilson score establishes a lower bound for a confidence interval on miner success rates given low sample or population sizes. For example, if a miner responds successfuly 8 of 9 times, it's lower bound on success rate is roughly `0.8701769999681506`. In other words `wilson_score(8,9) == 0.8701769999681506`, suggesting that 87% of the time, this miner is expected to succeed at a minimum on average. This calculation is used as the lower bound to determine tier eligibility and establishes a \"minimium trust level\" for the miner. If a miner continues to have failed challenges, the wilson score lower bound will cross below the current tier threshold, and when the tiers are recalculated, that miner will move down a tier (e.g. `Diamond -> Gold`.)\n",
    "\n",
    "Miners are also incentivized to be available, and are punshed for not being online, (e.g. failed `n` pings) by the `monitor` protocol. Periodically (every `m` steps) validators ping a set of `k` miners (default is 40), and after 5 successive failed attempts to ping a specific UID, that miner will be negatively rewarded for unavailability.\n",
    "\n",
    "Here is a simplified implementation of the `monitor` protocol:\n",
    "```python\n",
    "async def monitor(self):\n",
    "    \"\"\"\n",
    "    Monitor all UIDs by ping and keep track of how many failures\n",
    "    occur. If a UID fails too many times, remove it from the\n",
    "    list of UIDs to ping.\n",
    "    \"\"\"\n",
    "    # Ping current subset of UIDs, keep track of who failed\n",
    "    query_uids = await get_available_query_miners(self, k=40)\n",
    "    _, failed_uids = await ping_uids(self, query_uids)\n",
    "\n",
    "    down_uids = []\n",
    "    for uid in failed_uids:\n",
    "        self.monitor_lookup[uid] += 1\n",
    "        if self.monitor_lookup[uid] > 5:\n",
    "            self.monitor_lookup[uid] = 0\n",
    "            # If threshold reached this round, negatively reward \n",
    "            down_uids.append(uid)\n",
    "\n",
    "    if down_uids:\n",
    "        # Negatively reward miners who are down.\n",
    "        rewards = torch.zeros(len(down_uids), dtype=torch.float32)\n",
    "\n",
    "        for i, uid in enumerate(down_uids):\n",
    "            rewards[i] = MONITOR_FAILURE_REWARD # typically -0.005\n",
    "\n",
    "        # Update moving averaged scores given new observations based on alpha\n",
    "        scattered_rewards: torch.FloatTensor = self.moving_averaged_scores.scatter(\n",
    "            0, down_uids, rewards\n",
    "        )\n",
    "        alpha: float = 0.05\n",
    "        self.moving_averaged_scores: torch.FloatTensor = alpha * scattered_rewards + (\n",
    "            1 - alpha\n",
    "        ) * self.moving_averaged_scores\n",
    "\n",
    "```\n",
    "\n",
    "- \n",
    "    - Case 3: Difference in latency.\n",
    "        - Then you can show that miners in Q1 generally response in faster time rather then miners in Q3.\n",
    "\n",
    "Yes, miners with lower latency are rewarded higher. There is a modified sigmoid function that is applied to the normalized response times. The sigmoid is adjusted to the `max` response time per query group. This scales such that the laggards in the group (bottom, right tail) are rewarded less than those in the left tail (faster response times).\n",
    "\n",
    "For example, if a group of miners respond with times (pre-sorted):\n",
    "```python\n",
    "proc_times = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "sigm_factor = sigmoid_normalize(proc_times, max(proc_times) * 2)\n",
    "sigm_factor\n",
    "> [0.9752, 0.9652, 0.9514, 0.9324, 0.9067, 0.8726, 0.8284, 0.7729, 0.7057, 0.6283]\n",
    "```\n",
    "\n",
    "Then rewards would be based on the `max_time` of `0.9` and multiplied by the initial rewards vector which is based on tiers.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "tiers = [\"Bronze\", \"Bronze\", \"Gold\", \"Diamond\", \"Bronze\", \"Silver\", \"Silver\", \"Super Saiyan\", \"Gold\", \"Bronze\"]\n",
    "rewards_vector = [0.6, 0.6, 0.8, 0.9, 0.6, 0.7, 0.7, 1.0, 0.8, 0.6]\n",
    "\n",
    "initial_rewards = rewards_vector * sigm_factor\n",
    "> [0.5851, 0.5791, 0.7611, 0.8392, 0.544, 0.6108, 0.5799, 0.7729, 0.5646, 0.3769]\n",
    "\n",
    "\n",
    "# Now if success or failer determines pos, neg, or zero reward:\n",
    "verified = [True, True, False, True, False, False, True, True, True, False]\n",
    "final_rewards = []\n",
    "for i,v in enumerate(verified):\n",
    "    final_rewards.append(initial_rewards[i] * (1 if v else 0))\n",
    "    \n",
    "final_rewards\n",
    "> [0.5851, 0.5791, 0.0, 0.8392, 0.0, 0.0, 0.5799, 0.7729, 0.5646, 0.0]\n",
    "```\n",
    "\n",
    "\n",
    "An example calculation is below for illustration:\n",
    "\n",
    "```python\n",
    "def adjusted_sigmoid_inverse(x, steepness=1, shift=0):\n",
    "    \"\"\"\n",
    "    Inverse of the adjusted sigmoid function.\n",
    "\n",
    "    This function is a modified version of the sigmoid function that is shifted to\n",
    "    the right by a certain amount but inverted such that low completion times are\n",
    "    rewarded and high completions dimes are punished relative to the batch.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(steepness * (x - shift)))\n",
    "\n",
    "def calculate_sigmoid_params(timeout):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid parameters based on the timeout value.\n",
    "\n",
    "    Args:\n",
    "    - timeout (float): The current timeout value.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the 'steepness' and 'shift' values for the current timeout.\n",
    "    \"\"\"\n",
    "    base_timeout = 1\n",
    "    base_steepness = 7\n",
    "    base_shift = 0.3\n",
    "\n",
    "    # Calculate the ratio of the current timeout to the base timeout\n",
    "    ratio = timeout / base_timeout\n",
    "\n",
    "    # Calculate steepness and shift based on the pattern\n",
    "    steepness = base_steepness / ratio\n",
    "    shift = base_shift * ratio\n",
    "\n",
    "    return steepness, shift\n",
    "\n",
    "def sigmoid_normalize(process_times, max_time):\n",
    "\n",
    "    # Center the completion times around 0 for effective sigmoid scaling\n",
    "    centered_times = process_times - np.mean(process_times)\n",
    "\n",
    "    # Calculate steepness and shift based on maximum time in the batch\n",
    "    steepness, shift = calculate_sigmoid_params(max_time)\n",
    "\n",
    "    # Apply adjusted sigmoid function to scale the times\n",
    "    return adjusted_sigmoid_inverse(centered_times, steepness, shift)\n",
    "\n",
    "# Main scaling function for reward latency. This modifies the initial tier based rewards further for more granularity\n",
    "async def scale_rewards(uids, responses):\n",
    "    max_time = max(\n",
    "        [\n",
    "            response.dendrite.process_time for response in responses\n",
    "            if response.dendrite.process_time is not None\n",
    "        ] or [1] # nobody responded successfully\n",
    "    )\n",
    "\n",
    "    sorted_axon_times = get_sorted_response_times(uids, responses, max_time=max_time)\n",
    "\n",
    "    # Extract only the process times\n",
    "    process_times = [proc_time for _, proc_time in sorted_axon_times]\n",
    "\n",
    "    # Apply logarithmic scaling to data sizes\n",
    "    log_data_sizes_np = np.log1p(data_sizes)\n",
    "\n",
    "    # Normalize the response times by data size (unit time)\n",
    "    data_normalized_process_times = np.asarray(np.array(process_times) / log_data_sizes_np)\n",
    "\n",
    "    # Normalize the response times\n",
    "    normalized_times = sigmoid_normalize(data_normalized_process_times, max(data_normalized_process_times) * 2)\n",
    "\n",
    "    # Create a dictionary mapping UIDs to normalized times\n",
    "    uid_to_normalized_time = {\n",
    "        uid: normalized_time\n",
    "        for (uid, _), normalized_time in zip(sorted_axon_times, normalized_times)\n",
    "    }\n",
    "\n",
    "    # Scale the data size-scaled rewards with normalized times\n",
    "    time_scaled_rewards = torch.tensor(\n",
    "        [   # tier reward * latency based normalized reward\n",
    "            rewards[i] * uid_to_normalized_time[uid]\n",
    "            for i, uid in enumerate(uids)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Final normalization if needed\n",
    "    rescale_factor = torch.sum(rewards) / torch.sum(time_scaled_rewards)\n",
    "    bt.logging.trace(f\"Rescale factor: {rescale_factor}\")\n",
    "    scaled_rewards = [reward * rescale_factor for reward in time_scaled_rewards]\n",
    "    return scaled_rewards\n",
    "\n",
    "\n",
    "# Usage:\n",
    "uids = ...\n",
    "responses = await dendrite(...)\n",
    "\n",
    "scaled_rewards = scale_rewards(uids, responses)\n",
    "\n",
    "```\n",
    "\n",
    "- \n",
    "    - Case 4: Please provide your own justification if the reasons above dosen't fit.\n",
    "\n",
    "In short, miners are incentivized to be (1) available, (2) performant, (3) correct, and (4) consistent and trustworthy over time. This creates an incentive landscape that is multidimensional and has degrees fo freedom such that miners are able to ascend and descend the tier system based on their participation over time in an organic fashion. The challenge is to take a binary outcome (do you have the data or not?) and turn it into a gradated landscape with a pseudo-linear incentive distribution curve.\n",
    "\n",
    "Incentive typically ranges from 350 -> 528. After immunity (2 days) miners either cross this incentive threshold and compete or drop off and are deregistered below ~350 level. The curve is smoothed out by the additions to the reward mechanism so that there are not distinct levels or groups by tier. The goal is to avoid a step function and allow foster a granular competitive landscape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) (If applicable) Miner landscape\n",
    "- How many unique responses can we get from the network and how many miners are giving the same responses. It is perfectly fine even if all of the miners respond the same thing.\n",
    "N/A: Miners are expected to respond successfully to challenges for only data they posses, and does not make sense to project the same request to all miners.\n",
    "\n",
    "> (1) Send the same request to all miners over the SN\n",
    ">\n",
    "> (2) Check the number of unique responses  \n",
    "> \n",
    "> (3) Check the number of miners giving the same response\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (D) (If applicable) Demonstrate the effectiveness of the scoring mechanism.\n",
    "- If you are using a reward/penalty model: \n",
    "    - Please load the reward or penalty model one by one and then show that the reward of a good response > the reward of a bad response\n",
    "    - Please also allow us to customise the input of the reward model\n",
    "\n",
    "    > (1) Load the reward/penalty model one by one \n",
    "    >\n",
    "    > (2) Define the good/bad response\n",
    "    >\n",
    "    > (3) Score the response with the model\n",
    "\n",
    "N/A No explicit ML reward model is used.\n",
    "\n",
    "- Otherwise, you may just give a brief explanation to how does your scoring mechanism works.\n",
    "\n",
    "### Algorithm\n",
    "Multi-dimensional reward mechanism across 4 main axes:\n",
    "(1) Availability: Is the miner reachable when requested? Incentivizes uptime.\n",
    "(2) Speed (performant): how fast was the normalized response time?\n",
    "(3) Correctness (reliable): did the proof succeed? y/n\n",
    "(4) Trustworthiness (correctness over time): Tier definitions encapsulate consistenty and trustworthiness over time. A miner that rarely fails challenges and faithfully returns data when requested will receive an initial higher proportional reward.\n",
    "\n",
    "The main drivers behind the reward mechanism are to model meritocratic systems as seen in the academic realm and in the business world that scales trust across time and observation of output. For example, we create various \"tiers\" in education, Undergraduate, Masters, PhD, Post Doc, or in software, Junior Engineer, Senior Engineer, Principal Engineer, Staff Engineer, etc where each subsequent level achieved has proportionally greater expecetations on performance and qualification of the individual. \n",
    "\n",
    "However, performance of the individual must match the expectations of the pedigree, and behavior that is inconsistent with a given level will be adjusted. If, for example, a Junior Engineer proves their output is substantial and over time completes projects that provide value, that engineer will be promoted to Senior Engineer over time, and pontentially beyond. Conversely, a Principal Engineer that consistently underperforms expectations will be demoted to a role with lower expectations until able to prove otherwise. The same logic applies to miners in FileTAO (SN21).\n",
    "\n",
    "Tier (class) mobility is at the heart of the mechanism, and provides a balance between competition and trust, where over time competitiveness breeds a degree of trust on which we can associate a degree of reliability with a given entity (or miner).\n",
    "\n",
    "### Example Store Reward\n",
    "```python\n",
    "\n",
    "miner_uids = [0, 1, 2, 3, 4]\n",
    "miner_tier = [\"Bronze\", \"Silver\", \"Gold\", \"Diamond\", \"Super Saiyan\"]\n",
    "tier_miner_rewards = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "response_times = [0.15, 0.12, 0.19, 0.11, 0.3]\n",
    "response_factor = sigmoid_normalize(response_times, max(response_times) * 2)\n",
    "top_2_miner_uids = [1, 3] # UIDs that returned fastest\n",
    "verified_responses = [True, True, True, True, True] # Assume all respond successfully\n",
    "\n",
    "rewards = [None for _ in range(len(miner_uids))]\n",
    "for uid in miner_uids:\n",
    "    rewards[uid] = tier_miner_rewards[uid] * response_factor[uid] if verified_responses[uid] else 0.0\n",
    "\n",
    "rewards\n",
    "> [0.5491, 0.6571, 0.6971, 0.8506, 0.6524]\n",
    "\n",
    "# Bump top 2 miners\n",
    "TIER_BOOSTS = {\n",
    "    \"Super Saiyan\": 1.02, # 2%  -> 1.02\n",
    "    \"Diamond\": 1.05,      # 5%  -> 0.945\n",
    "    \"Gold\": 1.1,          # 10% -> 0.88\n",
    "    \"Silver\": 1.15,       # 15% -> 0.805\n",
    "    \"Bronze\": 1.2,        # 20% -> 0.72\n",
    "}\n",
    "for uid in top_2_miner_uids:\n",
    "    rewards[uid] *= TIER_BOOSTS[miner_tier[uid]]\n",
    "\n",
    "# boosts:    15%             5%\n",
    "rewards \n",
    "> [0.5491, 0.7557, 0.6971, 0.8931, 0.6524]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance Protocol (Data Preservation)\n",
    "A critical component of the subnet mechanism is the rebalance protocol wherein data is recovered and minimum data redundancy is restored when miners are (1) deregistered, or (2) unavailable after a certain amount of time.\n",
    "\n",
    "The general algorithm is as follows:\n",
    "(1) Block watcher checks each block's events for `NeuronRegistered` events for `netuid=21`.\n",
    "(2) When found, all client data for replaced `hotkey` is gathered from the `k-1` redundant miners and distributed to 2 new miners, thus *increasing* redundancy factor for the subnet as a whole.\n",
    "(3) All challenge data is wiped for that `hotkey` and purged from the index. This way miners who deregister are not responsible for old challenge data they may no longer possess.\n",
    "(4) The tier and statistics are reset for that `hotkey`.\n",
    "\n",
    "Pseudo-code below:\n",
    "```python\n",
    "\n",
    "async def rebalance_data_for_hotkey(\n",
    "    database, k: int, source_hotkey: str, hotkey_replaced: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Get all data from a given miner/hotkey and rebalance it to other miners.\n",
    "\n",
    "    (1) Get all data from a given miner/hotkey.\n",
    "    (2) Find out which chunks belong to full files, ignore the rest (challenges)\n",
    "    (3) Distribute the data that belongs to full files to other miners.\n",
    "    \"\"\"\n",
    "    metadata = await get_metadata_for_hotkey(source_hotkey, database)\n",
    "\n",
    "    miner_hashes = list(metadata)\n",
    "\n",
    "    rebalance_hashes = []\n",
    "    for _hash in miner_hashes:\n",
    "        if await is_file_chunk(_hash, database):\n",
    "            rebalance_hashes.append(_hash)\n",
    "\n",
    "    if hotkey_replaced:\n",
    "        # Reset miner statistics\n",
    "        await register_miner(source_hotkey, database)\n",
    "        # Update index for full and chunk hashes for retrieve\n",
    "        # Iterate through ordered metadata for all full hashses this miner had\n",
    "        async for file_key in database.scan_iter(\"file:*\"):\n",
    "            file_key = file_key.decode(\"utf-8\")\n",
    "            file_hash = file_key.split(\":\")[1]\n",
    "            # Get all ordered metadata for this file\n",
    "            ordered_metadata = await get_ordered_metadata(file_hash, database)\n",
    "            for chunk_metadata in ordered_metadata:\n",
    "                # Remove the dropped miner from the chunk metadata\n",
    "                await remove_hotkey_from_chunk(\n",
    "                    chunk_metadata, source_hotkey, database\n",
    "                )\n",
    "        # Purge challenge hashes so new miner doesn't get hosed\n",
    "        await purge_challenges_for_hotkey(source_hotkey, database)\n",
    "\n",
    "    # Take each data that needs to be spread and send it to `k` (usually 2) new miners.\n",
    "    for _hash in rebalance_hashes:\n",
    "        await rebalance_data_for_hash(data_hash=_hash, k=k)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "await rebalance_data_for_hotkey(\n",
    "    database, k, hotkey, hotkey_replaced=True\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Distribute Protocol\n",
    "\n",
    "Periodically, in the `forward` step, data is `distribute`d similar to the `rebalance` protocol, but by increasing data redundancy so that data never becomes \"stale\" on specific miners. A particular client data hash is chosen at random then spread to 2 new miners. \n",
    "\n",
    "Simplified implementation below:\n",
    "```python\n",
    "\n",
    "async def distribute_data(self, k: int):\n",
    "    \"\"\"\n",
    "    Distribute data storage among miners by migrating data from a set of miners to others.\n",
    "\n",
    "    Parameters:\n",
    "    - k (int): The number of miners to query and distribute data from.\n",
    "\n",
    "    Returns:\n",
    "    - A report of the rebalancing process.\n",
    "    \"\"\"\n",
    "\n",
    "    full_hashes = [key async for key in self.database.scan_iter(\"file:*\")]\n",
    "\n",
    "    full_hash = random.choice(full_hashes)\n",
    "    ordered_metadata = await get_ordered_metadata(full_hash, self.database)\n",
    "\n",
    "    # Get the hotkeys/uids to query\n",
    "    exclude_uids = set()\n",
    "    for chunk_metadata in ordered_metadata:\n",
    "        uids = [\n",
    "            self.metagraph.hotkeys.index(hotkey)\n",
    "            for hotkey in chunk_metadata[\"hotkeys\"]\n",
    "            if hotkey\n",
    "            in self.metagraph.hotkeys\n",
    "        ]\n",
    "        # Collect all uids for later exclusion\n",
    "        exclude_uids.update(uids)\n",
    "\n",
    "    # Use primitives to retrieve and store all the chunks:\n",
    "    retrieved_data, retrieved_payload = await retrieve_broadband(self, full_hash)\n",
    "\n",
    "    # Pick new miners to store data on that are not previously utilized, thereby increasing redundancy and data decentralization.\n",
    "    await store_broadband(\n",
    "        self,\n",
    "        retrieved_data,\n",
    "        encryption_payload=retrieved_payload,\n",
    "        exclude_uids=list(exclude_uids),\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (E) (If applicable) Show the dataset that was used by the validator.\n",
    " > (1) Load the dataset \n",
    " > \n",
    " > (2) Show the first 10 samples of the dataset \n",
    "\n",
    "N/A No dataset. Validation is performed on both random and intermingled user data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (F) (If applicable) Demonstrate the use of any API and/or links to a frontend.\n",
    "\n",
    "Dashboard: COMING SOON!: \n",
    "[Dash Screenshot](Dashboard.png)\n",
    "\n",
    "Front End: COMING SOON!: \n",
    "[Front End Screenshot](FrontEnd.png)\n",
    "\n",
    "### High Level API\n",
    "API example (python):\n",
    "```python\n",
    "import time\n",
    "import bittensor as bt\n",
    "from typing import List, Optional\n",
    "from storage.api import store, retrieve\n",
    "bt.trace()\n",
    "\n",
    "# Example usage\n",
    "async def store_things() -> Tuple[str, List[str]]:\n",
    "    # setup wallet and subtensor connection\n",
    "    wallet = bt.wallet()\n",
    "    subtensor = bt.subtensor(\"test\")\n",
    "\n",
    "    # Store some data using the validator set\n",
    "    data = b\"This is a test of the API high level abstraction\"\n",
    "    print(\"Storing data on the Bittensor testnet.\")\n",
    "    cid, hotkeys = await store(data, wallet, subtensor, netuid=22)\n",
    "    print(\"Stored {} with {} hotkeys\".format(cid, hotkeys))\n",
    "    return cid, hotkeys\n",
    "\n",
    "async def retrieve_things(cid: str, hotkeys: Optional[List[str]] = None) -> bytes:\n",
    "    print(\"Now retrieving data with CID: \", cid)\n",
    "    # Retrieve some data by querying validator set that contains said data\n",
    "    data = await retrieve(cid, wallet, subtensor, netuid=22, hotkeys=hotkeys)\n",
    "    return data\n",
    "\n",
    "\n",
    "cid, hotkeys = await store_things()\n",
    "\n",
    "time.sleep(5)\n",
    "data = await retrieve_things(cid, hotkeys)\n",
    "print(f\"Retrieved data: {data}!\")\n",
    "```\n",
    "\n",
    "\n",
    "### Low level API\n",
    "This uses the bittensor subnets api, slightly modified for our use case here.\n",
    "\n",
    "Here is how we utilize the Subnets API abstract class to extend two simple fucntions, prepare and process:\n",
    "\n",
    "Subnets API:\n",
    "```python\n",
    "\n",
    "class Subnet21API(ABC):\n",
    "    def __init__(self, wallet: \"bt.wallet\"):\n",
    "        self.wallet = wallet\n",
    "        self.dendrite = bt.dendrite(wallet=wallet)\n",
    "\n",
    "    async def __call__(self, *args, **kwargs):\n",
    "        return await self.query_api(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_synapse(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Prepare the synapse-specific payload.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_responses(self, responses: List[Union[\"bt.Synapse\", Any]]) -> Any:\n",
    "        \"\"\"\n",
    "        Process the responses from the network.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def query_api(\n",
    "        self,\n",
    "        axons: Union[bt.axon, List[bt.axon]],\n",
    "        deserialize: Optional[bool] = False,\n",
    "        timeout: Optional[int] = 12,\n",
    "        n: Optional[float] = 0.1,\n",
    "        uid: Optional[int] = None,\n",
    "        **kwargs: Optional[Any],\n",
    "    ) -> Any:\n",
    "        ... # Implementation not shown for brevity. Find src in `storage/api/base.py`\n",
    "```\n",
    "\n",
    "E.g. How do we prepare synapse for querying? And what do we do with the reponses when receieved?\n",
    "\n",
    "```python\n",
    "class StoreUserAPI(Subnet21API):\n",
    "    def __init__(self, wallet: \"bt.wallet\"):\n",
    "        super().__init__(wallet)\n",
    "        self.netuid = 21\n",
    "\n",
    "    def prepare_synapse(\n",
    "        self, data: bytes, encrypt=False, ttl=60 * 60 * 24 * 30, encoding=\"utf-8\"\n",
    "    ) -> StoreUser:\n",
    "        data = bytes(data, encoding) if isinstance(data, str) else data\n",
    "        encrypted_data, encryption_payload = (\n",
    "            encrypt_data(data, self.wallet) if encrypt else (data, \"{}\")\n",
    "        )\n",
    "        expected_cid = generate_cid_string(encrypted_data)\n",
    "        encoded_data = base64.b64encode(encrypted_data)\n",
    "\n",
    "        synapse = StoreUser(\n",
    "            encrypted_data=encoded_data,\n",
    "            encryption_payload=encryption_payload,\n",
    "            ttl=ttl,\n",
    "        )\n",
    "\n",
    "        return synapse\n",
    "\n",
    "    def process_responses(self, responses: List[Union[\"bt.Synapse\", Any]], return_failures: bool = False) -> Union[str, List[str]]:\n",
    "        success = False\n",
    "        successful_hotkeys = []\n",
    "        failure_modes = {\"code\": [], \"message\": []}\n",
    "        for response in responses:\n",
    "            if response.dendrite.status_code != 200:\n",
    "                failure_modes[\"code\"].append(response.dendrite.status_code)\n",
    "                failure_modes[\"message\"].append(response.dendrite.status_message)\n",
    "                continue\n",
    "\n",
    "            stored_cid = (\n",
    "                response.data_hash.decode(\"utf-8\")\n",
    "                if isinstance(response.data_hash, bytes)\n",
    "                else response.data_hash\n",
    "            )\n",
    "            success = True\n",
    "            bt.logging.debug(f\"Successfully stored CID {stored_cid} with hotkey {response.axon.hotkey}\")\n",
    "            successful_hotkeys.append(response.axon.hotkey)\n",
    "\n",
    "        if success:\n",
    "            bt.logging.info(\n",
    "                f\"Stored data on the Bittensor network with CID {stored_cid}\"\n",
    "            )\n",
    "        else:\n",
    "            bt.logging.error(\n",
    "                f\"Failed to store data. Response failure codes & messages {failure_modes}\"\n",
    "            )\n",
    "            stored_cid = \"\"\n",
    "\n",
    "        if return_failures:\n",
    "            return stored_cid, successful_hotkeys, failure_modes\n",
    "\n",
    "        return stored_cid, successful_hotkeys\n",
    "```\n",
    "\n",
    "Now we can use our lower level primitives to accomplish the same goal as the abstraction above in `store` and `receieve` functions.\n",
    "\n",
    "```python\n",
    "import bittensor as bt\n",
    "from storage.api import StoreUserAPI, RetrieveUserAPI, get_query_api_axons\n",
    "\n",
    "wallet = bt.wallet()\n",
    "store_handler = StoreUserAPI(wallet)\n",
    "\n",
    "# Fetch the axons of the available API nodes, or specify UIDs directly\n",
    "metagraph = bt.subtensor(\"test\").metagraph(netuid=22)\n",
    "all_axons = await get_query_api_axons(wallet=wallet, metagraph=metagraph)\n",
    "axons = random.choices(all_axons, k=3)\n",
    "\n",
    "# Store some data!\n",
    "raw_data = b\"Hello FileTao! This is a test of storing data on SN21.\"\n",
    "\n",
    "bt.logging.info(f\"Storing data {raw_data} on the Bittensor testnet.\")\n",
    "cid, hotkeys = await store_handler(\n",
    "    axons=axons,\n",
    "    # any arguments for the proper synapse\n",
    "    data=raw_data,\n",
    "    encrypt=False, # optionally encrypt the data with your bittensor wallet\n",
    "    ttl=60 * 60 * 24 * 30,\n",
    "    encoding=\"utf-8\",\n",
    "    uid=None,\n",
    "    timeout=60,\n",
    ")\n",
    "print(\"Stored {} with {} hotkeys\".format(cid, hotkeys))\n",
    "\n",
    "time.sleep(5)\n",
    "bt.logging.info(f\"Now retrieving data with CID: {cid}\")\n",
    "retrieve_handler = RetrieveUserAPI(wallet)\n",
    "rdata = await retrieve_handler(\n",
    "    axons=axons,\n",
    "    # Arugmnts for the proper synapse\n",
    "    cid=cid,\n",
    "    timeout=60,\n",
    ")\n",
    "print(rdata)\n",
    "assert raw_data == rdata\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "To test these directly, you can run `storage/api/examples.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO QUERY MINERS USING A VALIDATOR HOTKEY\n",
    "import base64\n",
    "import numpy as np\n",
    "import bittensor as bt\n",
    "from redis import asyncio as aioredis\n",
    "from Crypto.Random import get_random_bytes\n",
    "from storage import protocol\n",
    "from storage.shared.utils import get_redis_password\n",
    "from storage.validator.database import *\n",
    "from storage.shared.ecc import (\n",
    "    hash_data,\n",
    "    setup_CRS,\n",
    "    ecc_point_to_hex,\n",
    ")\n",
    "\n",
    "\n",
    "data = b\"Some bytes data to store on the network!\"\n",
    "subtensor = bt.subtensor(\"finney\")\n",
    "metagraph = subtensor.metagraph(netuid=21, lite=False)\n",
    "\n",
    "# Grab top 10% of miners by incentive\n",
    "def get_top_miner_uids(n=0.1):\n",
    "    top_i = np.quantile(metagraph.I, 1 - n)\n",
    "    uids = metagraph.uids[metagraph.I > top_i]\n",
    "    return [uid.item() for uid in uids]\n",
    "\n",
    "axons = [metagraph.axons[uid] for uid in get_top_miner_uids()]\n",
    "\n",
    "# Setup CRS (common reference string) for this round of validation\n",
    "curve = \"P-256\"\n",
    "g, h = setup_CRS(curve=curve)\n",
    "\n",
    "# Hash the data\n",
    "data_hash = hash_data(data)\n",
    "\n",
    "# Convert to base64 for compactness\n",
    "b64_encrypted_data = base64.b64encode(data).decode(\"utf-8\")\n",
    "\n",
    "# Create Store synapse\n",
    "synapse = protocol.Store(\n",
    "    encrypted_data=b64_encrypted_data,\n",
    "    curve=curve,\n",
    "    g=ecc_point_to_hex(g),\n",
    "    h=ecc_point_to_hex(h),\n",
    "    seed=get_random_bytes(32).hex(),  # 256-bit seed\n",
    "    ttl=12345, # how many seconds before miners can safely delete\n",
    ")\n",
    "\n",
    "# NOTE: Must be a registered validator hotkey to work.\n",
    "name = \"default\"\n",
    "hotkey = \"default\"\n",
    "wallet = bt.wallet(name, hotkey)\n",
    "dendrite = bt.dendrite(wallet)\n",
    "\n",
    "responses = await dendrite(\n",
    "    axons,\n",
    "    synapse,\n",
    "    deserialize=False\n",
    ")\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO STORE ON TESTNET\n",
    "\n",
    "import time\n",
    "import random\n",
    "import bittensor as bt\n",
    "from storage.api import store, retrieve\n",
    "from storage.api import StoreUserAPI, RetrieveUserAPI, get_query_api_axons\n",
    "bt.trace()\n",
    "\n",
    "# setup wallet and subtensor connection\n",
    "wallet_name = \"default\"\n",
    "wallet_hotkey = \"default\"\n",
    "wallet = bt.wallet(wallet_name, wallet_hotkey)\n",
    "subtensor = bt.subtensor(\"test\")\n",
    "\n",
    "# Store some data and retrieve it\n",
    "data = b\"This is a test of the API high level abstraction\"\n",
    "\n",
    "print(\"Storing data on the Bittensor testnet.\")\n",
    "cid, hotkeys = await store(data, wallet, subtensor, netuid=22)\n",
    "\n",
    "print(\"Stored {} with {} hotkeys\".format(cid, hotkeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO RETRIEVE ON TESTNET\n",
    "print(\"Now retrieving data with CID: \", cid)\n",
    "rdata = await retrieve(cid, wallet, subtensor, netuid=22, hotkeys=hotkeys)\n",
    "print(rdata)\n",
    "assert data == rdata, \"Data does not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO STORE ON MAINNET (Assumes at least 1 API node is running with --open_access for whitelisting all keys.)\n",
    "\n",
    "# setup wallet and subtensor connection\n",
    "wallet_name = \"default\"\n",
    "wallet_hotkey = \"default\"\n",
    "wallet = bt.wallet(wallet_name, wallet_hotkey)\n",
    "subtensor = bt.subtensor(\"finney\")\n",
    "\n",
    "# Store some data and retrieve it\n",
    "data = b\"This is a test of the API high level abstraction\"\n",
    "\n",
    "print(\"Storing data on the Bittensor testnet.\")\n",
    "cid, hotkeys = await store(data, wallet, subtensor, netuid=21)\n",
    "\n",
    "print(\"Stored {} with {} hotkeys\".format(cid, hotkeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO RETRIEVE ON MAINNET\n",
    "print(\"Now retrieving data with CID: \", cid)\n",
    "rdata = await retrieve(cid, wallet, subtensor, netuid=21, hotkeys=hotkeys)\n",
    "print(rdata)\n",
    "assert data == rdata, \"Data does not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consent: Do you want this demo notebook to be public? Yes/No \n",
    "Yes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
